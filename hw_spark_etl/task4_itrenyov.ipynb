{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>539</td><td>application_1678651132715_1554</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vk-edu-s202346b22c11-Head-0.mcs.local:8088/proxy/application_1678651132715_1554/\">Link</a></td><td><a target=\"_blank\" href=\"http://vk-edu-s202346b22c11-Worker-0.mcs.local:8042/node/containerlogs/container_e02_1678651132715_1554_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "user = \"itrenyov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "current_dt = \"2023-03-01\"\n",
    "\n",
    "city = \"/user/{}/data/data3/rosstat/city.csv\".format(user)\n",
    "product = \"/user/{}/data/data3/rosstat/product.csv\".format(user)\n",
    "products_for_stat = \"/user/{}/data/data3/rosstat/products_for_stat.csv\".format(user)\n",
    "price_path = \"/user/{}/data/data3/rosstat/price\".format(user)\n",
    "\n",
    "demography_path = \"/user/{}/data/data3/ok/coreDemography\".format(user)\n",
    "countries = \"/user/{}/data/data3/ok/geography/countries.csv\".format(user)\n",
    "rs_city = \"/user/{}/data/data3/ok/geography/rs_city.csv\".format(user)\n",
    "\n",
    "# Путь до результата\n",
    "output_path_price_stat = \"/user/{}/task4/price_stat\".format(user)\n",
    "output_path_ok_dem = \"/user/{}/task4/ok_dem\".format(user)\n",
    "output_path_product_stat = \"/user/{}/task4/product_stat\".format(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приводим все к DataFrame и даем понятные названия столбцам\n",
    "cityDF = spark.read.csv(city, sep=';').select(\n",
    "    sf.col(\"_c0\").cast(StringType()).alias(\"city\"),\n",
    "    sf.col(\"_c1\").cast(IntegerType()).alias(\"city_id\")\n",
    ")\n",
    "\n",
    "productDF = spark.read.csv(product, sep=';').select(\n",
    "    sf.col(\"_c0\").cast(StringType()).alias(\"product\"),\n",
    "    sf.col(\"_c1\").cast(IntegerType()).alias(\"product_id\")\n",
    ")\n",
    "\n",
    "productStatDF = spark.read.csv(products_for_stat, sep=';').select(\n",
    "    sf.col(\"_c0\").cast(IntegerType()).alias(\"product_id\")\n",
    ")\n",
    "\n",
    "countriesDF = spark.read.csv(countries, sep=',').select(\n",
    "    sf.col(\"_c0\").cast(IntegerType()).alias(\"city_id\"),\n",
    "    sf.col(\"_c1\").cast(StringType()).alias(\"city_name\")\n",
    ")\n",
    "\n",
    "rsCityDF = spark.read.csv(rs_city, sep='\\t').select(\n",
    "    sf.col(\"_c0\").cast(LongType()).alias(\"ok_city_id\"),\n",
    "    sf.col(\"_c1\").cast(IntegerType()).alias(\"rs_city_id\")\n",
    ")\n",
    "\n",
    "priceDF = spark.read.csv(price_path, sep=';').select(\n",
    "    sf.col(\"_c0\").cast(LongType()).alias(\"rs_city_id\"),\n",
    "    sf.col(\"_c1\").cast(IntegerType()).alias(\"product_id\"),\n",
    "    sf.col(\"_c2\").alias(\"price\")\n",
    ")\n",
    "priceDF = priceDF.withColumn(\"price\", sf.regexp_replace(\"price\", ',', '.').cast(\"float\")) \n",
    "\n",
    "demographyDF = spark.read.csv(demography_path, sep='\\t').select(\n",
    "    sf.col(\"_c0\").cast(IntegerType()).alias(\"user_id\"),\n",
    "    sf.col(\"_c1\").cast(LongType()).alias(\"create_date\"),\n",
    "    sf.col(\"_c2\").cast(ShortType()).alias(\"birth_date\"),\n",
    "    sf.col(\"_c3\").cast(IntegerType()).alias(\"gender\"),\n",
    "    sf.col(\"_c4\").cast(LongType()).alias(\"id_country\"),\n",
    "    sf.col(\"_c5\").cast(LongType()).alias(\"id_city\"),\n",
    "    sf.col(\"_c6\").cast(IntegerType()).alias(\"login_region\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_stat = (\n",
    "    priceDF.alias(\"left\")\n",
    "    .join(productStatDF, priceDF.product_id == productStatDF.product_id, \"inner\")\n",
    "    .select(sf.col(\"left.rs_city_id\"), sf.col(\"left.product_id\"), sf.col(\"left.price\"))\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(\n",
    "        sf.min(sf.col(\"price\")).cast(DecimalType(20,2)).alias(\"min_price\"),\n",
    "        sf.max(sf.col(\"price\")).cast(DecimalType(20,2)).alias(\"max_price\"),\n",
    "        sf.avg(sf.col(\"price\")).cast(DecimalType(20,2)).alias(\"avg_price\"),\n",
    "    )\n",
    "    .orderBy([\"product_id\"], ascending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_stat = spark.read.option(\"header\", \"true\").csv(output_path_price_stat, sep=';')\n",
    "\n",
    "# UPD age calculation: https://towardsdatascience.com/how-to-accurately-calculate-age-in-bigquery-999a8417e973\n",
    "\n",
    "ok_dem  = (\n",
    "    priceDF\n",
    "    .join(\n",
    "        price_stat,\n",
    "        [priceDF.product_id == price_stat.product_id, priceDF.price > price_stat.avg_price],\n",
    "        \"inner\",\n",
    "    )\n",
    "    .select(sf.col('rs_city_id')).alias(\"rich_cities\")\n",
    "    .distinct()\n",
    "    .join(cityDF, sf.col('rich_cities.rs_city_id') == cityDF.city_id, \"inner\")\n",
    "    .select(sf.col(\"city\"), sf.col(\"rs_city_id\"))\n",
    "    .join(rsCityDF, sf.col('rich_cities.rs_city_id') == rsCityDF.rs_city_id, \"inner\")\n",
    "    .select(sf.col('city'), sf.col('ok_city_id'))\n",
    "    .join(demographyDF, sf.col('ok_city_id') == demographyDF.id_city, 'inner')\n",
    "    .select(\n",
    "        sf.col('city'),\n",
    "        sf.col('birth_date'),\n",
    "        sf.col('gender'),\n",
    "    )\n",
    "    .groupBy(sf.col('city'))\n",
    "    .agg(\n",
    "        sf.count(sf.col('gender')).alias('user_cnt'),\n",
    "        sf.count(sf.when(sf.col('gender') == 1, True)).alias('men_cnt'),\n",
    "        sf.count(sf.when(sf.col('gender') == 2, True)).alias('women_cnt'),\n",
    "        sf.avg(sf.datediff(sf.lit(current_dt), sf.from_unixtime(sf.col('birth_date') * 24 * 60 * 60)) / 365.25)\n",
    "        .cast(IntegerType()).alias('age_avg'),\n",
    "    )\n",
    "    .select(\n",
    "        sf.col('city'),\n",
    "        sf.col('user_cnt'),\n",
    "        sf.col('age_avg'),\n",
    "        sf.col('men_cnt'),\n",
    "        sf.col('women_cnt'),\n",
    "        (sf.col('men_cnt') / sf.col('user_cnt')).cast(DecimalType(20,2)).alias('men_share'),\n",
    "        (sf.col('women_cnt') / sf.col('user_cnt')).cast(DecimalType(20,2)).alias('women_share'),\n",
    "    )\n",
    "    .orderBy(sf.col('user_cnt'), ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ok_dem = spark.read.option(\"header\", \"true\").csv(output_path_ok_dem, sep=';')\n",
    "stats = ok_dem.agg(\n",
    "    sf.max(sf.col(\"age_avg\")),\n",
    "    sf.min(sf.col(\"age_avg\")),\n",
    "    sf.max(sf.col(\"men_share\")),\n",
    "    sf.max(sf.col(\"women_share\")),\n",
    ").collect()[0]\n",
    "\n",
    "product_stat = (\n",
    "    ok_dem\n",
    "    .where(\n",
    "        (sf.col(\"age_avg\") == stats[0]) |\n",
    "        (sf.col(\"age_avg\") == stats[1]) |\n",
    "        (sf.col(\"men_share\") == stats[2]) |\n",
    "        (sf.col(\"women_share\") == stats[3])\n",
    "    )\n",
    "    .select(sf.col(\"city\")).alias(\"suitable_cities\")\n",
    "    .join(cityDF, sf.col(\"suitable_cities.city\") == cityDF.city, \"inner\")\n",
    "    .select(sf.col(\"suitable_cities.city\"), sf.col(\"city_id\"))\n",
    "    .join(priceDF, sf.col(\"city_id\") == priceDF.rs_city_id, \"inner\")\n",
    "    .select(sf.col(\"city\"), sf.col(\"product_id\"), sf.col(\"price\")).alias(\"stat_ok\")\n",
    "    .join(productDF, sf.col(\"stat_ok.product_id\") == productDF.product_id, \"inner\")\n",
    "    .select(sf.col(\"city\"), sf.col(\"product\"), sf.col(\"price\"))\n",
    "    .withColumn('cheap', sf.min('price').over(Window.partitionBy('city')))\n",
    "    .withColumn('expensive', sf.max('price').over(Window.partitionBy('city')))\n",
    "    .select(\n",
    "        sf.col(\"city\").alias(\"city_name\"),\n",
    "        sf.col('price'),\n",
    "        sf.when(sf.col(\"price\") == sf.col(\"cheap\"), sf.col(\"product\")).alias('min_product'),\n",
    "        sf.when(sf.col(\"price\") == sf.col(\"expensive\"), sf.col(\"product\")).alias('max_product'),\n",
    "        (sf.col(\"expensive\") - sf.col(\"cheap\")).alias(\"price_difference\")\n",
    "    )\n",
    "    .where(\n",
    "        (sf.col(\"min_product\").isNotNull()) | \n",
    "        (sf.col(\"max_product\").isNotNull())\n",
    "    )\n",
    "    .groupBy(\"city_name\")\n",
    "    .agg(\n",
    "        sf.last(\"max_product\", True).alias('most_expensive_product_name'), \n",
    "        sf.last(\"min_product\", True).alias('cheapest_product_name'),\n",
    "        sf.last(\"price_difference\", True).alias('price_difference'),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение результата на hdfs\n",
    "(price_stat.repartition(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"sep\", \";\")\n",
    " .csv(output_path_price_stat)\n",
    ")\n",
    "\n",
    "(ok_dem.repartition(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"sep\", \";\")\n",
    " .csv(output_path_ok_dem)\n",
    ")\n",
    "\n",
    "(product_stat.repartition(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"sep\", \";\")\n",
    " .csv(output_path_product_stat) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После работы обязательно отключаем спарк и отдаем ресурсы!\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
